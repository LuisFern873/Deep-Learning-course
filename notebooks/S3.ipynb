{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "<img src=\"../images/image8.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input at time step $t$: $x_t$\n",
    "- Ouput at time step $t$: $y_t$\n",
    "- Hidden state at time step $t$: $h_t$\n",
    "\n",
    "$$h_t=\\tanh(W_{hh}^T h_{t-1} + W_{xh}^T x_t)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$y_t=W_{hy}^T h_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "hidden_size = 10\n",
    "num_layers = 1\n",
    "\n",
    "x = torch.randn(seq_len, batch_size, input_size)\n",
    "h0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.randn(num_layers, batch_size, hidden_size)\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente respecto a W_hx: -0.029792867600917816\n",
      "Gradiente respecto a W_hh: -0.0\n",
      "Gradiente respecto a W_yh: -0.01750630885362625\n"
     ]
    }
   ],
   "source": [
    "W_hx = torch.tensor(0.5, requires_grad=True)\n",
    "W_hh = torch.tensor(0.8, requires_grad=True)  \n",
    "W_yh = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "x_t = torch.tensor(1.0)  # Entrada en el tiempo t\n",
    "h_t_minus_1 = torch.tensor(0.0)  # Estado oculto inicial\n",
    "y_t = torch.tensor(0.5)  # Valor verdadero (target)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "h_t = torch.tanh(W_hx * x_t + W_hh * h_t_minus_1)\n",
    "y_pred = W_yh * h_t\n",
    "loss = 0.5 * (y_pred - y_t) ** 2\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Gradientes calculados\n",
    "print(\"Gradiente respecto a W_hx:\", W_hx.grad.item())\n",
    "print(\"Gradiente respecto a W_hh:\", W_hh.grad.item())\n",
    "print(\"Gradiente respecto a W_yh:\", W_yh.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/image9.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short-term memory\n",
    "\n",
    "- Resuelve Vanishing and Exploding Gradients Problems.\n",
    "- Flujo ininterrumpido de la gradiente.\n",
    "- $C_t$: estado celular (almacena información de largo plazo).\n",
    "\n",
    "\n",
    "- $f_t = σ(w_f [h_{t-1}, x_t] + b_f )$, $i_t = ...$, $o_t = ...$\n",
    "- $\\hat{C_{t}} = \\tanh(w_c[h_{t-1},x_t]+b_c)$\n",
    "\n",
    "$$C_t = f_t ⊙ C_{t-1} + i_t ⊙ \\hat{C_{t}}$$\n",
    "\n",
    "$$h_t = o_t ⊙ \\tanh(C_t)$$\n",
    "\n",
    "- Forget gate ($f_t$): Decide cuánto del estado anterior se retiene.\n",
    "- Input gate ($i_t$): Controla cuánto de la nueva información se agrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 20])\n",
      "torch.Size([1, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# seq_len: cantidad de pasos de tiempo (t)\n",
    "# x: [seq_len, batch_size, input_size]\n",
    "seq_len = 3\n",
    "input_size = 10\n",
    "batch_size = 5\n",
    "\n",
    "x = torch.randn(seq_len, batch_size, input_size)\n",
    "\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size) # initial hidden state\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size) # initial cell state\n",
    "\n",
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "# torch.Size([3, 5, 20])\n",
    "print(h0.shape)\n",
    "print(hn.shape)\n",
    "print(cn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2440, -0.2586],\n",
      "        [-0.2549, -0.2535],\n",
      "        [-0.2108, -0.3007],\n",
      "        [-0.1166, -0.0822],\n",
      "        [-0.2338, -0.2223]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.fc(hn[-1]) # last hidden layer\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "seq_len = 3\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "\n",
    "model = SentimentLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "y = model(x) # forward\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wf = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Forget gate\n",
    "        self.wi = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Input gate\n",
    "        self.wo = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Output gate\n",
    "        self.wc = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Candidate cell state\n",
    "\n",
    "    def forward(self, xt, ht, ct):  # Input, previous hidden state, previous cell state\n",
    "        zt = torch.cat([xt, ht], dim=1)  # Concatenating input and hidden state\n",
    "\n",
    "        ft = torch.sigmoid(self.wf(zt))  # Forget gate\n",
    "        it = torch.sigmoid(self.wi(zt))  # Input gate\n",
    "        ot = torch.sigmoid(self.wo(zt))  # Output gate\n",
    "        cct = torch.tanh(self.wc(zt))    # Candidate cell state\n",
    "\n",
    "        ct = ft * ct + it * cct  # Update cell state\n",
    "        ht = ot * torch.tanh(ct)  # Update hidden state\n",
    "\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 𝑧𝑡 = 𝜎(𝜔𝑧 ∙ [ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑧)\n",
    "# 𝑟𝑡 = 𝜎(𝜔𝑟 ∙ [ℎ𝑡−1, 𝑥𝑡] + 𝑏𝑟)\n",
    "# ℎℎ𝑡 = tanh(𝜔ℎ ∙ [𝑟𝑡 ⨀ ℎ𝑡−1, 𝑥𝑡] + 𝑏ℎ)\n",
    "# ℎ𝑡 = (1 − 𝑧𝑡) ⨀ ℎ𝑡−1 + 𝑧𝑡 ⨀ ℎℎ𝑡\n",
    "    \n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wz = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Update gate\n",
    "        self.wr = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Reset gate\n",
    "        self.wh = nn.Linear(input_size + hidden_size, hidden_size, bias=True)  # Candidate hidden state\n",
    "\n",
    "    def forward(self, xt, ht):  # Input, previous hidden state\n",
    "        xt_ht = torch.cat([xt, ht], dim=1)\n",
    "\n",
    "        zt = torch.sigmoid(self.wz(xt_ht))  # Update gate\n",
    "        rt = torch.sigmoid(self.wr(xt_ht))  # Reset gate\n",
    "\n",
    "        h_tilde = torch.tanh(self.wh(torch.cat([xt, rt * ht], dim=1)))  # Candidate hidden state\n",
    "\n",
    "        ht = (1 - zt) * ht + zt * h_tilde  # Final hidden state\n",
    "\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMcell(nn.Module):\n",
    "    def __init__(self, in_chan, n_hidden):\n",
    "        super(LSTMcell, self).__init__()\n",
    "        self.in_chan = in_chan\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.W = nn.Linear(in_chan + n_hidden, 4*n_hidden, bias=True)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, xt, ht, ct):\n",
    "\n",
    "        zt = torch.cat((xt, ht), dim=1)\n",
    "\n",
    "        fox = self.W(zt)\n",
    "\n",
    "        it, ft, ot, cct = torch.chunk(fox, 4, dim=1)\n",
    "        \n",
    "\n",
    "        ct = ft*ct + it*cct\n",
    "        ht = ot*self.Tanh(ct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
