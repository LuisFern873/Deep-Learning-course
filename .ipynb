{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_chan):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.MLP  = nn.Conv2d(in_chan, in_chan, kernel_size=1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgPool    = torch.mean(x, dim=(2, 3), keepdim=True)\n",
    "        maxPool, _ = torch.max(x, dim=2, keepdim=True)  \n",
    "        maxPool, _ = torch.max(maxPool, dim=3, keepdim=True)  \n",
    "        out = self.MLP(avgPool) + self.MLP(maxPool)\n",
    "        return self.sigmoid(out) * x\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv    = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgPool    = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxPool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avgPool, maxPool], dim=1)\n",
    "        out = self.conv(concat)\n",
    "        return self.sigmoid(out) * x\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_chan, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.chAttn = ChannelAttention(in_chan)\n",
    "        self.spAttn = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.chAttn(x)\n",
    "        x = self.spAttn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttentionDaddy(nn.Module):\n",
    "    def __init__(self, in_chan):\n",
    "        super(ChannelAttentionDaddy, self).__init__()\n",
    "        self.MLP  = nn.Conv2d(in_chan, in_chan, kernel_size=1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgPool    = torch.mean(x, dim=(2, 3), keepdim=True)\n",
    "        maxPool, _ = torch.max(x, dim=2, keepdim=True)  \n",
    "        maxPool, _ = torch.max(maxPool, dim=3, keepdim=True)  \n",
    "        out = self.MLP(avgPool) + self.MLP(maxPool)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttentionDaddy(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttentionDaddy, self).__init__()\n",
    "        self.conv    = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgPool    = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxPool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avgPool, maxPool], dim=1)\n",
    "        out = self.conv(concat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAMDaddy(nn.Module):\n",
    "    def __init__(self, in_chan, kernel_size=7):\n",
    "        super(CBAMDaddy, self).__init__()\n",
    "        self.chAttn = ChannelAttention(in_chan)\n",
    "        self.spAttn = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.chAttn(x) * x\n",
    "        x = self.spAttn(x) * x \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 6, 6])\n",
      "tensor([[[[8.1796e-02, 6.5391e-01, 2.8656e-03, 5.2295e+00, 1.4008e+00,\n",
      "           1.3271e+00],\n",
      "          [7.3064e-02, 1.2798e-01, 3.0845e+00, 2.8438e-01, 4.7552e-01,\n",
      "           7.0303e+00],\n",
      "          [1.3514e+00, 3.4308e-03, 4.1171e-01, 2.4986e-04, 5.0205e-01,\n",
      "           5.6473e-01],\n",
      "          [2.1673e-03, 5.5378e-02, 2.8053e-06, 2.3386e-02, 1.7142e-01,\n",
      "           2.6164e-01],\n",
      "          [4.8052e-04, 3.3477e-02, 1.9566e-01, 5.7302e-02, 9.6979e-02,\n",
      "           6.7784e-03],\n",
      "          [9.7828e+00, 1.0728e-01, 2.3751e+00, 2.5471e-04, 5.5607e+00,\n",
      "           6.1168e-07]],\n",
      "\n",
      "         [[7.7405e-02, 2.8208e-02, 4.2619e-02, 6.9947e-01, 1.0849e-01,\n",
      "           9.3995e-03],\n",
      "          [2.8000e-08, 1.4173e+00, 7.8529e-04, 4.4211e-01, 3.3867e-03,\n",
      "           4.9019e-04],\n",
      "          [3.0995e-02, 1.8595e+00, 6.9131e-01, 7.1287e-04, 1.3922e-02,\n",
      "           5.1789e-08],\n",
      "          [8.5691e-02, 2.5090e-02, 2.1725e-02, 2.0520e-01, 2.9485e-01,\n",
      "           3.4463e-02],\n",
      "          [1.0213e-03, 1.6909e-01, 1.9868e-05, 1.7384e+00, 1.5518e-03,\n",
      "           3.9495e-01],\n",
      "          [2.1983e-01, 3.0064e-03, 3.8076e+00, 1.2920e-03, 1.4567e+00,\n",
      "           3.9862e-01]],\n",
      "\n",
      "         [[2.0837e-07, 1.3081e-02, 2.8674e-02, 6.3029e-01, 1.1571e-03,\n",
      "           7.4719e-04],\n",
      "          [2.2012e-02, 1.2439e-02, 2.5974e-03, 3.3506e-02, 3.6702e-03,\n",
      "           1.6528e-03],\n",
      "          [1.6025e-03, 5.0187e-02, 1.9132e-01, 3.9866e-13, 1.9291e-03,\n",
      "           7.7276e-05],\n",
      "          [1.3085e-03, 1.6988e-05, 2.0225e-06, 1.8428e-04, 1.9675e-03,\n",
      "           4.7735e-05],\n",
      "          [4.8242e-05, 3.9794e-04, 3.2334e-05, 5.9115e-03, 1.9613e-03,\n",
      "           4.4988e-08],\n",
      "          [4.3937e-03, 5.2484e-02, 1.0461e-02, 5.6698e-12, 1.6053e-05,\n",
      "           1.1936e-08]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 3, 6, 6)\n",
    "\n",
    "model = CBAMDaddy(3)\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RepVGGBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.conv3 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(channels)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.bn0   = nn.BatchNorm2d(channels)\n",
    "        self.ReLU  = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_3x3 = self.bn3(self.conv3(x))\n",
    "        out_1x1 = self.bn1(self.conv1(x))\n",
    "        out_id  = self.bn0(x)\n",
    "        out = self.ReLU(out_3x3 + out_1x1 + out_id, inplace=True)\n",
    "        return out\n",
    "\n",
    "class RepVGGBlockDeploy(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(RepVGGBlockDeploy, self).__init__()\n",
    "        self.fused_conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fused_conv(x))\n",
    "\n",
    "# Fusiona una conv con su BN\n",
    "def fuse_conv_bn(conv, bn):\n",
    "    W = conv.weight         # [channels, channels, k, k]\n",
    "    gamma = bn.weight\n",
    "    beta  = bn.bias\n",
    "    mean  = bn.running_mean\n",
    "    var   = bn.running_var\n",
    "    eps   = bn.eps\n",
    "    std = torch.sqrt(var + eps)\n",
    "    \n",
    "    W_fused = W * (gamma / std).reshape(-1, 1, 1, 1)\n",
    "    b_fused = beta - mean * (gamma / std)\n",
    "    return W_fused, b_fused\n",
    "\n",
    "# Expande un kernel 1x1 a 3x3\n",
    "def expand_1x1_to_3x3(W1):\n",
    "    outC, inC, _, _ = W1.shape\n",
    "    W1_expanded = torch.zeros((outC, inC, 3, 3), dtype=W1.dtype, device=W1.device)\n",
    "    W1_expanded[:, :, 1, 1] = W1[:, :, 0, 0]\n",
    "    return W1_expanded\n",
    "\n",
    "# Crea el kernel identidad y lo fusiona con su BN\n",
    "def fuse_identity_bn(channels, bn):\n",
    "    # Crear el kernel identidad\n",
    "    I = torch.eye(channels).view(channels, channels, 1, 1).to(bn.weight.device)\n",
    "\n",
    "    # Extraer parámetros de BN:\n",
    "    gamma = bn.weight \n",
    "    beta  = bn.bias \n",
    "    mean  = bn.running_mean\n",
    "    var   = bn.running_var\n",
    "    eps   = bn.eps\n",
    "\n",
    "    # Calcular la desviación estándar:\n",
    "    std = sqrt(var + eps)\n",
    "\n",
    "    # Fusionar\n",
    "    W_fused = I * (gamma / std).reshape([channels, 1, 1, 1])\n",
    "    b_fused = beta - mean * (gamma / std)\n",
    "\n",
    "    return W_fused, b_fused\n",
    "\n",
    "# Función para convertir un bloque de entrenamiento a versión evaluacion\n",
    "def repVGG_convert_block(block):\n",
    "    channels = block.conv3.in_channels \n",
    "    \n",
    "    # Fusionar rama 3x3\n",
    "    W3, b3 = fuse_conv_bn(block.conv3, block.bn3)\n",
    "    \n",
    "    # Fusionar rama 1x1 y expandir a 3x3\n",
    "    W1, b1 = fuse_conv_bn(block.conv1, block.bn1)\n",
    "    W1_3x3 = expand_1x1_to_3x3(W1)\n",
    "    \n",
    "    # Fusionar rama identidad y expandir a 3x3\n",
    "    W_id, b_id = fuse_identity_bn(channels, block.bn0)\n",
    "    W_id_3x3 = expand_1x1_to_3x3(W_id)\n",
    "    \n",
    "    # Sumar los tres kernels y biases\n",
    "    W_fused = W3 + W1_3x3 + W_id_3x3\n",
    "    b_fused = b3 + b1 + b_id\n",
    "    \n",
    "    # Emplear los pesos fusionados\n",
    "    deploy_block = RepVGGBlockDeploy(channels)\n",
    "    deploy_block.fused_conv.weight.data.copy_(W_fused)\n",
    "    deploy_block.fused_conv.bias.data.copy_(b_fused)\n",
    "    return deploy_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
